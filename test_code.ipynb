{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ipynb.fs.full.functions' (/home/bkotryna/ML_practice/allrecipes_project/functions.ipynb)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import public things\n",
    "\n",
    "# general / random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynb\n",
    "import re # for string parsing / editing\n",
    "import string # for string parsing / editing\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# for html\n",
    "import requests # for getting html off the web\n",
    "from bs4 import BeautifulSoup # for parsing html\n",
    "import json\n",
    "\n",
    "# for ML\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import snowballstemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# import functions from my functions file\n",
    "import ipynb.fs.full.functions as funcs\n",
    "\n",
    "# update a module if it's been edited\n",
    "# (this is just going around a jupyter feature where simply re-importing doesn't do anything)\n",
    "# https://support.enthought.com/hc/en-us/articles/204469240-Jupyter-IPython-After-editing-a-module-changes-are-not-effective-without-kernel-restart\n",
    "import importlib\n",
    "importlib.reload(funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## directly from Kenny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('broccoli', 3), ('eggs', 2), ('sugar', 2), ('water', 2), ('mix', 1)]\n",
      "Number of words in vocab = 11\n",
      "\n",
      "Component 0\n",
      "broccoli : 1.291\n",
      "water : 0.798\n",
      "made : 0.493\n",
      "soup : 0.493\n",
      "over : 0.305\n",
      "\n",
      "Component 1\n",
      "eggs : 0.914\n",
      "sugar : 0.914\n",
      "goes : 0.513\n",
      "well : 0.513\n",
      "mix : 0.401\n",
      "\n",
      "Component 0:\n",
      "The recipe that most strongly embodies this component is:\n",
      "Broccoli soup is made with water and broccoli\n",
      "\n",
      "Component 1:\n",
      "The recipe that most strongly embodies this component is:\n",
      "Sugar goes well with eggs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "data = np.array([\n",
    "        [4.5, 'Mix eggs with sugar'],\n",
    "        [3.0, 'Pour water over the broccoli'],\n",
    "        [4.8, 'Sugar goes well with eggs'],\n",
    "        [2.8, 'Broccoli soup is made with water and broccoli']\n",
    "        ])\n",
    "\n",
    "STOP_WORDS = ['with', 'the', 'and', 'is']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n",
    "vectors = vectorizer.fit_transform(data[:,1]).todense()\n",
    "\n",
    "# Print vocab items with their frequencies, sorted in descending order by frequency\n",
    "word_and_frequency_tuples = []\n",
    "for word, index in vectorizer.vocabulary_.items():\n",
    "    frequency_of_current_word = vectors[:, index].sum()\n",
    "    word_and_frequency_tuples.append((word, frequency_of_current_word))\n",
    "by_freq = sorted(word_and_frequency_tuples, key=lambda x: x[1], reverse=True)\n",
    "print(by_freq[0:5])\n",
    "\n",
    "vocab_size = len(word_and_frequency_tuples)\n",
    "print('Number of words in vocab = {}'.format(vocab_size))\n",
    "\n",
    "indices_to_words = {index : word for word, index in vectorizer.vocabulary_.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=2)\n",
    "nmf_projections = nmf.fit_transform(vectors)\n",
    "\n",
    "\n",
    "# Inspect what each component is about (i.e. what the topic is)\n",
    "for component_id in range(len(nmf.components_)):\n",
    "    print('\\nComponent {}'.format(component_id))\n",
    "    nmf.components_[component_id]\n",
    "    word_indices_in_descending_order_by_importance_for_component = sorted(\n",
    "        range(vocab_size),\n",
    "        key=(lambda word_index: nmf.components_[component_id, word_index]),\n",
    "        reverse=True\n",
    "    )\n",
    "    for index in word_indices_in_descending_order_by_importance_for_component[:5]:\n",
    "        word = indices_to_words[index]\n",
    "        importance = nmf.components_[component_id, index]\n",
    "        print('{} : {:.3f}'.format(word, importance))\n",
    "\n",
    "# For each component, find the sentence that has the largest projection along that component\n",
    "for component_id in range(len(nmf.components_)):\n",
    "    id_of_recipe_with_greatest_projection_along_component = max(\n",
    "        range(len(data)),\n",
    "        key=(lambda recipe_index: nmf_projections[recipe_index, component_id])\n",
    "    )\n",
    "    print('\\nComponent {}:'.format(component_id))\n",
    "    print('The recipe that most strongly embodies this component is:')\n",
    "    print(data[id_of_recipe_with_greatest_projection_along_component, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4.8', 'Sugar goes well with eggs'], dtype='<U45')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[id_of_recipe_with_greatest_projection_along_component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix\n",
      "eggs\n",
      "sugar\n",
      "pour\n",
      "water\n",
      "over\n",
      "broccoli\n",
      "goes\n",
      "well\n",
      "soup\n",
      "made\n"
     ]
    }
   ],
   "source": [
    "for word, index in vectorizer.vocabulary_.items():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate = 0.100\n",
      "Feature Cakeyness: importance = 0.524\n",
      "Feature Veganness: importance = 0.476\n",
      "Feature Frenchness: importance = 0.000\n",
      "Mean abs error on train = 0.000\n",
      "Mean abs error on validation = 0.450\n",
      "\n",
      "Learning rate = 0.050\n",
      "Feature Veganness: importance = 0.622\n",
      "Feature Cakeyness: importance = 0.378\n",
      "Feature Frenchness: importance = 0.000\n",
      "Mean abs error on train = 0.014\n",
      "Mean abs error on validation = 0.436\n",
      "\n",
      "Learning rate = 0.025\n",
      "Feature Veganness: importance = 0.524\n",
      "Feature Cakeyness: importance = 0.476\n",
      "Feature Frenchness: importance = 0.000\n",
      "Mean abs error on train = 0.191\n",
      "Mean abs error on validation = 0.259\n",
      "\n",
      "Learning rate = 0.010\n",
      "Feature Veganness: importance = 0.520\n",
      "Feature Cakeyness: importance = 0.480\n",
      "Feature Frenchness: importance = 0.000\n",
      "Mean abs error on train = 0.878\n",
      "Mean abs error on validation = 0.428\n",
      "\n",
      "Learning rate = 0.005\n",
      "Feature Veganness: importance = 0.506\n",
      "Feature Cakeyness: importance = 0.494\n",
      "Feature Frenchness: importance = 0.000\n",
      "Mean abs error on train = 1.454\n",
      "Mean abs error on validation = 1.004\n",
      "\n",
      "Learning rate = 0.001\n",
      "Feature Cakeyness: importance = 0.528\n",
      "Feature Veganness: importance = 0.472\n",
      "Feature Frenchness: importance = 0.000\n",
      "Mean abs error on train = 2.172\n",
      "Mean abs error on validation = 1.722\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "feature_names = ['Cakeyness', 'Veganness', 'Frenchness']\n",
    "train_features = np.array([\n",
    "      [4.5, 0.1, 5.0],\n",
    "      [8.2, 0.2, 0.0],\n",
    "      [0.3, 5.6, 0.0],\n",
    "      [0.4, 8.9, 5.0]\n",
    "    ])\n",
    "train_ratings = np.array([\n",
    "      4.9,\n",
    "      4.9,\n",
    "      0.1,\n",
    "      0.1\n",
    "    ])\n",
    "\n",
    "validation_features = np.array([\n",
    "      [3.5, 0.1, 2.5],\n",
    "      [0.4, 6.9, 2.5]\n",
    "    ])\n",
    "validation_ratings = np.array([\n",
    "      4.2,\n",
    "      0.3\n",
    "    ])\n",
    "\n",
    "X_train = train_features\n",
    "y_train = train_ratings\n",
    "\n",
    "def train_and_evaluate_regressor(regressor):\n",
    "    regressor.fit(X_train, y_train)\n",
    "    features_and_importances = []\n",
    "    for feature_id in range(len(regressor.feature_importances_)):\n",
    "        feature_name = feature_names[feature_id]\n",
    "        feature_importance = regressor.feature_importances_[feature_id]\n",
    "        features_and_importances.append((feature_name, feature_importance))\n",
    "    features_and_importances.sort(key=(lambda pair: pair[1]), reverse=True)\n",
    "    for pair in features_and_importances[:5]:\n",
    "        print('Feature {}: importance = {:.3f}'.format(pair[0], pair[1]))\n",
    "    \n",
    "    train_predictions = regressor.predict(train_features)\n",
    "    validation_predictions = regressor.predict(validation_features)\n",
    "    mean_abs_error_on_train = mean_absolute_error(train_ratings, train_predictions)\n",
    "    mean_abs_error_on_validation = mean_absolute_error(validation_ratings, validation_predictions)\n",
    "    print('Mean abs error on train = {:.3f}'.format(mean_abs_error_on_train))\n",
    "    print('Mean abs error on validation = {:.3f}'.format(mean_abs_error_on_validation))\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "for learning_rate in [0.1, 0.05, 0.025, 0.010, 0.005, 0.001]:\n",
    "    print('\\nLearning rate = {:.3f}'.format(learning_rate))\n",
    "    regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=learning_rate)\n",
    "    train_and_evaluate_regressor(regressor)\n",
    "    \n",
    "    # High learning rates => overfitted\n",
    "    # Low learning rates => underfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NMF stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>apple pie</td>\n",
       "      <td>apples, pastry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>beef stew</td>\n",
       "      <td>beef, potatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>chicken nuggets</td>\n",
       "      <td>chicken, breadcrumbs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title           ingredients\n",
       "1334        apple pie        apples, pastry\n",
       "1456        beef stew        beef, potatoes\n",
       "1998  chicken nuggets  chicken, breadcrumbs"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(index=['1334','1456','1998'], columns=['title','ingredients'])\n",
    "df['title'] = ['apple pie', 'beef stew', 'chicken nuggets']\n",
    "df['ingredients'] = ['apples, pastry', 'beef, potatoes', 'chicken, breadcrumbs']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple pie'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['1334', 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple pie'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.at['1334','title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\n",
      "1456\n",
      "1998\n"
     ]
    }
   ],
   "source": [
    "for index in df.index:\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>apple pie</td>\n",
       "      <td>apples, pastry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>beef stew</td>\n",
       "      <td>beef, potatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>chicken nuggets</td>\n",
       "      <td>chicken, breadcrumbs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title           ingredients\n",
       "1334        apple pie        apples, pastry\n",
       "1456        beef stew        beef, potatoes\n",
       "1998  chicken nuggets  chicken, breadcrumbs"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nmf_S(col, df, nmf_df, n=4):\n",
    "    # input = df, column of interest, number of NMF components to keep\n",
    "    # output = augmented df that now contains n new columns, each corresponding to an NMF components.\n",
    "    # text from each row in the column of interest is expressed in terms of the NMF components\n",
    "    \n",
    "    print(f\"\\n************\\nNow working on column '{col}':\")\n",
    "    \n",
    "    # obtain data\n",
    "    # cell = a string\n",
    "    data = df  \n",
    "    display(data)\n",
    "    \n",
    "    # let's stem\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    # will generate a list with one item per recipe\n",
    "    # each item will be a string of stemmed words\n",
    "    data['stemmed'] = ''\n",
    "\n",
    "    for index in data.index:\n",
    "        print(index)\n",
    "        # stem\n",
    "        item = data.loc[index, col]\n",
    "        print(item)\n",
    "        item_stem = stemmer.stemWords(item.split())\n",
    "\n",
    "        # generate a single string per recipe\n",
    "        data_string = ' '.join(item_stem)\n",
    "\n",
    "        # remove strange quotation marks\n",
    "        # (Ig ideally would know how not to generate them in the first place)\n",
    "        data_string = data_string.replace(\"'\",\"\") \n",
    "        data_string = data_string.replace('\"','')\n",
    "\n",
    "        # append the string to df\n",
    "        data.at[index, 'stemmed'] = data_string\n",
    "    print('Stemming done') \n",
    "    \n",
    "    \n",
    "    data = df['stemmed'].to_numpy()\n",
    "    display(data)\n",
    "    \n",
    "    \n",
    "    # data = df[col].to_numpy()\n",
    "    # tokenise (make into a bag of words)\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(data).todense()\n",
    "    print(f\"vocabulary size: {vectors.shape[1]}\")\n",
    "    print('Tokenising done.')\n",
    "    \n",
    "    # Print vocab items with their frequencies, sorted in descending order by frequency\n",
    "    word_and_frequency_tuples = []\n",
    "    for word, index in vectorizer.vocabulary_.items():\n",
    "        frequency_of_current_word = vectors[:, index].sum()\n",
    "        word_and_frequency_tuples.append((word, frequency_of_current_word))\n",
    "    by_freq = sorted(word_and_frequency_tuples, key=lambda x: x[1], reverse=True)\n",
    "    print(f'Most freqeunt words are:\\n{by_freq[0:5]}')\n",
    "    vocab_size = len(word_and_frequency_tuples)\n",
    "    \n",
    "    # might be useful some time\n",
    "    indices_to_words = {index : word for word, index in vectorizer.vocabulary_.items()}\n",
    "\n",
    "    # do NMF\n",
    "    nmf = NMF(n_components=n)\n",
    "    nmf_projections = nmf.fit_transform(vectors)\n",
    "    #display(nmf_projections)\n",
    "    print('NMF transforming done.')\n",
    "\n",
    "    # Inspect individual components\n",
    "    for component_id in range(len(nmf.components_)):\n",
    "        print(f'\\n***\\nComponent {component_id} for {col}')\n",
    "        \n",
    "        ### Inspect what each component is about (i.e. what the topic is)\n",
    "        print('\\nMost important words are:')\n",
    "        \n",
    "        # BTW nmf.components_ is an np.array\n",
    "        # nmf.components_[component_id]\n",
    "        word_indices_in_descending_order_by_importance_for_component = sorted(\n",
    "            range(vocab_size),\n",
    "            key=(lambda word_index: nmf.components_[component_id, word_index]),\n",
    "            reverse=True\n",
    "        )\n",
    "        for index in word_indices_in_descending_order_by_importance_for_component[:5]:\n",
    "            word = indices_to_words[index]\n",
    "            importance = nmf.components_[component_id, index]\n",
    "            print('{} : {:.3f}'.format(word, importance))\n",
    "\n",
    "        ### find the sentence that has the largest projection along that component\n",
    "        id_of_recipe_with_greatest_projection_along_component = max(\n",
    "            range(len(data)),\n",
    "            key=(lambda recipe_index: nmf_projections[recipe_index, component_id])\n",
    "        )\n",
    "        print('\\nThe entry that most strongly embodies this component is:')\n",
    "        print(data[id_of_recipe_with_greatest_projection_along_component])\n",
    "\n",
    "    # generate column names for nmf df\n",
    "    col_names_list = []\n",
    "    for num in range(1, nmf_projections.shape[1] + 1):\n",
    "        col_name = f\"{col}_nmf_{num}\"\n",
    "        col_names_list.append(col_name)\n",
    "\n",
    "    # generate nmf df\n",
    "    our_col_nmf_df = pd.DataFrame(nmf_projections, columns=col_names_list)\n",
    "    our_col_nmf_df\n",
    "    print('\\n***\\nnp.array made into pd.df')\n",
    "\n",
    "    # set index to match recipe_id\n",
    "    our_col_nmf_df['recipe_id'] = df.index\n",
    "    our_col_nmf_df.set_index('recipe_id', inplace=True)\n",
    "    print(\"Index now reset back to recipe_id.\")\n",
    "    \n",
    "    return our_col_nmf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************\n",
      "Now working on column 'title':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>apple pie</td>\n",
       "      <td>apples, pastry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>beef stew</td>\n",
       "      <td>beef, potatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>chicken nuggets</td>\n",
       "      <td>chicken, breadcrumbs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title           ingredients\n",
       "1334        apple pie        apples, pastry\n",
       "1456        beef stew        beef, potatoes\n",
       "1998  chicken nuggets  chicken, breadcrumbs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\n",
      "apple pie\n",
      "1456\n",
      "beef stew\n",
      "1998\n",
      "chicken nuggets\n",
      "Stemming done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['appl pie', 'beef stew', 'chicken nugget'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 6\n",
      "Tokenising done.\n",
      "Most freqeunt words are:\n",
      "[('appl', 1), ('pie', 1), ('beef', 1), ('stew', 1), ('chicken', 1)]\n",
      "NMF transforming done.\n",
      "\n",
      "***\n",
      "Component 0 for title\n",
      "\n",
      "Most important words are:\n",
      "appl : 0.487\n",
      "pie : 0.487\n",
      "beef : 0.000\n",
      "chicken : 0.000\n",
      "nugget : 0.000\n",
      "\n",
      "The entry that most strongly embodies this component is:\n",
      "appl pie\n",
      "\n",
      "***\n",
      "Component 1 for title\n",
      "\n",
      "Most important words are:\n",
      "stew : 0.887\n",
      "beef : 0.812\n",
      "appl : 0.000\n",
      "chicken : 0.000\n",
      "nugget : 0.000\n",
      "\n",
      "The entry that most strongly embodies this component is:\n",
      "beef stew\n",
      "\n",
      "***\n",
      "Component 2 for title\n",
      "\n",
      "Most important words are:\n",
      "nugget : 1.831\n",
      "chicken : 1.831\n",
      "appl : 0.000\n",
      "beef : 0.000\n",
      "pie : 0.000\n",
      "\n",
      "The entry that most strongly embodies this component is:\n",
      "chicken nugget\n",
      "\n",
      "***\n",
      "Component 3 for title\n",
      "\n",
      "Most important words are:\n",
      "beef : 0.313\n",
      "stew : 0.182\n",
      "appl : 0.000\n",
      "chicken : 0.000\n",
      "nugget : 0.000\n",
      "\n",
      "The entry that most strongly embodies this component is:\n",
      "beef stew\n",
      "\n",
      "***\n",
      "np.array made into pd.df\n",
      "Index now reset back to recipe_id.\n"
     ]
    }
   ],
   "source": [
    "# generate a master nmf_df\n",
    "nmf_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "# try NMF\n",
    "col = 'title'\n",
    "our_col_nmf_df = make_nmf_S(col, df, nmf_df, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_nmf_1</th>\n",
       "      <th>title_nmf_2</th>\n",
       "      <th>title_nmf_3</th>\n",
       "      <th>title_nmf_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>2.051446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.007735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title_nmf_1  title_nmf_2  title_nmf_3  title_nmf_4\n",
       "recipe_id                                                    \n",
       "1334          2.051446     0.000000     0.000000     0.000000\n",
       "1456          0.000000     1.007735     0.000000     0.581224\n",
       "1998          0.000000     0.000000     0.546009     0.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_col_nmf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
