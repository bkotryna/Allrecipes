{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for allrecipes.com project\n",
    "\n",
    "Just so it's not all in one massive hard-to-navigate file.<br>\n",
    "Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import public things\n",
    "\n",
    "# general / random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynb\n",
    "import re # for string parsing / editing\n",
    "import string # for string parsing / editing\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# for html\n",
    "import requests # for getting html off the web\n",
    "from bs4 import BeautifulSoup # for parsing html\n",
    "import json\n",
    "\n",
    "# for ML\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import snowballstemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# html retrieval etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_recipe_links_from_html_file(page):\n",
    "    # find all recipe links in a website and save them into a list\n",
    "    # input = html page\n",
    "    # output = a list of links\n",
    "    \n",
    "    links_list = []\n",
    "\n",
    "    all_link_elements = page.find_all('a', attrs={'class':'card__titleLink manual-link-behavior'})\n",
    "\n",
    "    for element in all_link_elements:\n",
    "        actual_link = element.get('href')\n",
    "\n",
    "        if '/recipe/' in actual_link:\n",
    "            # don't add a link if it's already in the list\n",
    "            if actual_link not in links_list:\n",
    "                links_list.append(actual_link)\n",
    "            \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recipe_ids_from_all_links_from_html_file(page):\n",
    "    # find all recipe links in a website and save them into a list\n",
    "    # input = html page\n",
    "    # output = a list of links\n",
    "    \n",
    "    ids_list = []\n",
    "\n",
    "    all_link_elements = page.find_all('a', attrs={'class':'card__titleLink manual-link-behavior'})\n",
    "\n",
    "    for element in all_link_elements:\n",
    "        actual_link = element.get('href')\n",
    "\n",
    "        if '/recipe/' in actual_link:\n",
    "            # don't add a link if it's already in the list\n",
    "            recipe_id = actual_link.split('/recipe/')[1].split('/')[0]\n",
    "            if recipe_id not in ids_list:\n",
    "                ids_list.append(recipe_id)\n",
    "            \n",
    "    return ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_pages_from_a_list_of_links(links_list):\n",
    "    # retrieve all pages from a list of links\n",
    "    # input = a list of links\n",
    "    # output = a list of html pages + print the length of the list\n",
    "    \n",
    "    page_list = []\n",
    "\n",
    "    for link in links_list: # can do the whole links_list if brave / have time\n",
    "        retrieved = requests.get(link)\n",
    "        page = BeautifulSoup(retrieved.content)\n",
    "        page_list.append(page)\n",
    "\n",
    "    print(\"Length of the recipe list is: \", len(page_list))\n",
    "    \n",
    "    return page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html_pages_from_file(filename):\n",
    "    # open a file that has lots of html pages and generate a list with individual html pages as elements\n",
    "    # input = filename / location\n",
    "    # output = (1) list of html pages, page_list, (2) print length of the page list\n",
    "\n",
    "    page_list = []\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        file_content = file.read()\n",
    "        search_phrase = '<!DOCTYPE html>'\n",
    "        while search_phrase in file_content:\n",
    "            # extract a single html page\n",
    "            first_part = search_phrase\n",
    "            second_part = file_content.split(search_phrase)[1]\n",
    "            full_item = first_part + second_part\n",
    "            html_page = BeautifulSoup(full_item)\n",
    "            page_list.append(html_page)\n",
    "\n",
    "            # edit the remaining text to remove the extracted html content\n",
    "            file_content = file_content.split(full_item)[1:][0]\n",
    "                \n",
    "    print(\"Length of the page_list is \", len(page_list))\n",
    "    \n",
    "    return page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_collections_from_id_list(id_list):\n",
    "    # input = a list of numerical id's for pages on allrecipes.com\n",
    "    # output =\n",
    "    # (1) page_list = a list of retrieved html pages\n",
    "    # (2) save each html as a separate file\n",
    "    \n",
    "    print('Start retrieving collections:\\n')\n",
    "    \n",
    "    page_list = []\n",
    "    for one_id in id_list:\n",
    "        one_id = str(one_id)\n",
    "        \n",
    "        print(f'Now retrieving collection id {one_id}')\n",
    "        \n",
    "        base_url = 'https://www.allrecipes.com/recipes/'\n",
    "        full_url = f\"{base_url}{one_id}\"\n",
    "        retrieved = requests.get(full_url)\n",
    "        page = BeautifulSoup(retrieved.content)\n",
    "        page_list.append(page)\n",
    "\n",
    "        name_to_save = f'{one_id}'\n",
    "        with open(name_to_save, 'w') as f:\n",
    "            for page in page_list:\n",
    "                f.write(\"%s\\n\" % page)\n",
    "\n",
    "    print('\\nCollection retrieval now finished.\\n')\n",
    "    \n",
    "    return page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_a_collection_name_from_an_html_page(page):\n",
    "    # input = an html page for a recipe collection\n",
    "    # output = collection_name = the name of the recipe collection\n",
    "    \n",
    "    title = page.find('title').text\n",
    "    collection_name = title.split('Recipes')[0].strip()\n",
    "        \n",
    "    return collection_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_recipes_from_id_list(id_list):\n",
    "    # input = a list of numerical id's for pages on allrecipes.com\n",
    "    # output =\n",
    "    # (1) pages_retrieved = the number of retrieved html pages\n",
    "    # (2) save them as a file\n",
    "    \n",
    "    print('Start retrieving recipes:\\n')\n",
    "    \n",
    "    pages_tried = 0\n",
    "    pages_retrieved = 0\n",
    "    for one_id in id_list:\n",
    "        one_id = str(one_id)\n",
    "        base_url = 'https://www.allrecipes.com/recipe/'\n",
    "        full_url = f\"{base_url}{one_id}\"\n",
    "        retrieved = requests.get(full_url)\n",
    "        pages_tried += 1\n",
    "        print(f'\\n{one_id} - attempting to retrieve. That makes it {pages_tried} pages tried so far.')\n",
    "        \n",
    "        if retrieved.status_code == 200:\n",
    "            page = BeautifulSoup(retrieved.content)\n",
    "            name_to_save = f'{one_id}'\n",
    "            with open(name_to_save, 'a') as f:\n",
    "                f.write(\"%s\\n\" % page)\n",
    "            pages_retrieved += 1\n",
    "            print(f'Success => {pages_retrieved} retrieved total')\n",
    "        \n",
    "        # delay so web-crawling doesn't get blocked by the website etc\n",
    "        delay()\n",
    "    \n",
    "    print('\\nRecipe retrieval now finished.\\n')\n",
    "    \n",
    "    return pages_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract useful information from recipe html's: no json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_recipe_have_video(page):\n",
    "    # extract if the recipe has a video\n",
    "    # get a yes-or-no answer (1 or 0)\n",
    "    # input = html page\n",
    "    # output = 1 if video present, 0 if not\n",
    "\n",
    "    video_element = page.find('label', attrs={'class':'recipe-play-label'})\n",
    "    video_present = 0 # default\n",
    "    if video_element:\n",
    "        video_present = 1\n",
    "    else:\n",
    "        video_present = 0\n",
    "        \n",
    "    return video_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_of_photos(page):\n",
    "    # extract the number of photos the recipe has\n",
    "    # input = html page\n",
    "    # output = number of photos\n",
    "    \n",
    "    photo_count = 0 # default\n",
    "    photo_count_raw = page.find('a', attrs={'class':'ugc-ratings-link ugc-photos-link'})\n",
    "    if photo_count_raw:\n",
    "        photo_count_text = page.find('a', attrs={'class':'ugc-ratings-link ugc-photos-link'}).text.strip()\n",
    "\n",
    "        # only keep the digits\n",
    "        photo_count_number = re.sub(\"[^0-9]\", \"\", photo_count_text).strip()\n",
    "        photo_count = int(photo_count_number)\n",
    "        \n",
    "    return photo_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract useful information from recipe html's: use json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cautiously_populate_df(input_df, column_name):\n",
    "    # input = pandas df, name of the column of interest\n",
    "    # output =\n",
    "    # (either) if the column exists => value of the first row in the column of interest\n",
    "    # (or) if the column doesn't exist => number zero\n",
    "    \n",
    "    if column_name in input_df.columns:\n",
    "        cell_value = input_df[column_name][0]\n",
    "    else:\n",
    "        cell_value = 0\n",
    "    \n",
    "    return cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_info_from_json_on_page_to_df(page):\n",
    "    # input = BeautifulSoup html page of a recipe\n",
    "    # output = pandas df that contains information from json near the beginning of the html\n",
    "\n",
    "    recipe_info_df = pd.DataFrame()\n",
    "    \n",
    "    # get out a useful chunck of json from the recipe html\n",
    "    json_block = page.find('script', type='application/ld+json')\n",
    "    if json_block:\n",
    "        json_block = json.loads(json_block.string)\n",
    "\n",
    "        # extract some recipe parameters, eg cook time\n",
    "        recipe_info_df = pd.json_normalize(json_block[1], max_level=None)\n",
    "    \n",
    "    return recipe_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_info(recipe_info_df):\n",
    "    # input = recipe_info_df with raw data from json\n",
    "    # output = key_info df\n",
    "\n",
    "    # extract recipe_id\n",
    "    recipe_id = recipe_info_df['mainEntityOfPage'][0]\n",
    "    recipe_id = recipe_id.split('recipe/')[1].split('/')[0]\n",
    "    recipe_id = int(recipe_id)\n",
    "\n",
    "    # extract recipe title\n",
    "    recipe_title = recipe_info_df['name'][0]\n",
    "\n",
    "    # extract date of publication\n",
    "    date_published = recipe_info_df['datePublished'][0]\n",
    "    date_published = date_published.split('T')[0]\n",
    "\n",
    "    # extract description\n",
    "    recipe_description = recipe_info_df['description'][0]\n",
    "\n",
    "    # extract ratings info\n",
    "    avg_rating = cautiously_populate_df(recipe_info_df, 'aggregateRating.ratingValue')\n",
    "    ratings_no = cautiously_populate_df(recipe_info_df, 'aggregateRating.ratingCount')\n",
    "    \n",
    "    # extract official \"recipeCategories\"\n",
    "    recipe_cats = recipe_info_df['recipeCategory'][0]\n",
    "\n",
    "    # make a df to store all the information\n",
    "    key_info_df = pd.DataFrame(columns=['recipe_id', 'title', 'date_published', 'description',\n",
    "                                        'avg_rating', 'ratings_no', 'recipe_cats'])\n",
    "    key_info_df.loc[0] = [recipe_id, recipe_title, date_published, recipe_description,\n",
    "                          avg_rating, ratings_no, recipe_cats]\n",
    "    \n",
    "    return key_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_times(recipe_info_df):\n",
    "    # extract times\n",
    "    # input = recipe_info_df with raw data from json\n",
    "    # output = times_df with times in minutes\n",
    "\n",
    "    # set up time naming conversions\n",
    "    time_columns = ['prepTime', 'cookTime', 'totalTime']\n",
    "\n",
    "    times_dict = {}\n",
    "    for time_column in time_columns:\n",
    "\n",
    "        if time_column in list(recipe_info_df.columns):\n",
    "            # extract time in minutes\n",
    "            time = recipe_info_df[time_column][0]\n",
    "            time_markers = ['T', 'H', 'M']\n",
    "            if  isinstance(time, str) and all(markers in time for markers in time_markers):\n",
    "                time_h = int(time.split('T')[1].split('H')[0])\n",
    "                time_min = int(time.split('H')[1].split('M')[0])\n",
    "                time_in_min = time_h * 60 + time_min\n",
    "            else:\n",
    "                time_in_min = 0\n",
    "        else:\n",
    "            # if this type of time doesn't exist in our input data, let's put it as zero\n",
    "            time_in_min = 0\n",
    "\n",
    "        # append data to times_dict to store time information\n",
    "        times_dict[time_column] = time_in_min\n",
    "\n",
    "    # create a df to store information\n",
    "    times_df = pd.DataFrame(times_dict, index=[0])\n",
    "    \n",
    "    return times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ingredients(recipe_info_df):\n",
    "    # extract ingredients\n",
    "    # input = recipe_info_df with raw data from json\n",
    "    # output = ingredients_df with ingredients names in a list\n",
    "\n",
    "    ingredients_list = recipe_info_df['recipeIngredient'][0]\n",
    "    \n",
    "    # number of ingredients\n",
    "    ingredients_no = len(ingredients_list)\n",
    "\n",
    "    # extract just the names of ingredients\n",
    "    measurements = ['spoon', 'cup', 'pinch', 'package', 'ounce', ' kg ', ' g ', ' l ', ' ml ', 'pound', 'and']\n",
    "    ingredient_names = []\n",
    "\n",
    "    for item in ingredients_list:\n",
    "        # remove numbers\n",
    "        item_name = re.sub(r'[^-A-Za-z ]+', \"\", item)\n",
    "\n",
    "        # remove non-informative words\n",
    "        # BTW word cloud is an easy way to spot common non-informative words that have been left in\n",
    "        useless_words = ['spoon', 'cup', 'pinch', 'package', 'ounce', ' kg ', ' g ', ' l ', ' ml ', 'pound',\n",
    "                         'and','dash', 'dice', 'cube', 'chop', 'ground', 'drain', 'beat', 'or more', ' or ', 'to taste', 'taste',\n",
    "                        'cut', 'piece', 'slice', 'inch', 'into', 'grate', 'peel', 'large', 'medium', 'small']\n",
    "\n",
    "        item_name = ' '.join([i for i in item_name.split() if not any(w in i.lower() for w in useless_words)])\n",
    "        item_name = item_name.strip(string.punctuation).strip()\n",
    "\n",
    "        # add to list\n",
    "        ingredient_names.append(item_name)\n",
    "    \n",
    "    # make a df as output\n",
    "    ingredients_df = pd.DataFrame(columns=['ingredients_no', 'ingredient_names'])\n",
    "    ingredients_df.loc[0] = [ingredients_no, ingredient_names]\n",
    "\n",
    "    return ingredients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_method_steps(recipe_info_df):\n",
    "    # input = recipe_info_df\n",
    "    # output = steps_df, which contains information on:\n",
    "    # (1) the number of steps in the method\n",
    "    # (2) a string that contains all the steps joined together\n",
    "\n",
    "    # extract step information from recipes_info_df\n",
    "    steps_json = recipe_info_df['recipeInstructions'][0]\n",
    "    # de-json the steps into a pandas df\n",
    "    steps_json_df = pd.json_normalize(steps_json)\n",
    "    # join all steps into a list\n",
    "    steps_list = steps_json_df['text'].tolist()\n",
    "\n",
    "    # count number of steps\n",
    "    steps_no = len(steps_list)\n",
    "\n",
    "    # join all steps from the list into a string\n",
    "    steps_str = ' '.join(steps_list)\n",
    "    # remove newline characters\n",
    "    steps_str = steps_str.replace('\\n','')\n",
    "    \n",
    "    # count number of words\n",
    "    steps_words = steps_str.split()\n",
    "    steps_words_no = len(steps_words)\n",
    "\n",
    "    # make a steps_dict and then convert it into a mini df\n",
    "    steps_dict = {'steps_no': steps_no, 'steps_str': steps_str, 'steps_words_no': steps_words_no}\n",
    "    steps_df = pd.DataFrame(steps_dict, index=[0])\n",
    "    \n",
    "    return steps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nutritional_info(recipe_info_df):\n",
    "    # extract nutrition info\n",
    "    # input = recipe_info_df with raw data from json\n",
    "    # output = nutrition_df with lots of nutritional info\n",
    "\n",
    "    nutrition_columns = ['nutrition.calories',\n",
    "           'nutrition.carbohydrateContent', 'nutrition.cholesterolContent',\n",
    "           'nutrition.fatContent', 'nutrition.fiberContent',\n",
    "           'nutrition.proteinContent', 'nutrition.saturatedFatContent',\n",
    "           'nutrition.servingSize', 'nutrition.sodiumContent',\n",
    "           'nutrition.sugarContent', 'nutrition.transFatContent',\n",
    "           'nutrition.unsaturatedFatContent']\n",
    "\n",
    "    nutrition_df = recipe_info_df[nutrition_columns]\n",
    "    \n",
    "    return nutrition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stars_and_review_info(page):\n",
    "    # extract info about star rating and number of reviews\n",
    "    # input = BeautifulSoup html page of the recipe\n",
    "    # output = stars_and_reviews df with the information\n",
    "\n",
    "    # get number of votes for each star\n",
    "    # input = html page\n",
    "    # output = dictionary with keys = star names, values = number of ratings\n",
    "\n",
    "    star_names = []\n",
    "    star_votes = []\n",
    "\n",
    "    ratings = page.find_all('li', attrs={'class':'rating'})\n",
    "\n",
    "    # need to guard against there being no ratings at all\n",
    "    if ratings:\n",
    "\n",
    "        for one_rating in ratings:\n",
    "            name_raw = one_rating.find('span', attrs={'class':'rating-stars'}).text[0]\n",
    "            name = int(name_raw)\n",
    "            star_names.append(name)\n",
    "\n",
    "            vote_raw = one_rating.find('span', attrs={'class':'rating-count'}).text.strip()\n",
    "            vote = int(vote_raw)\n",
    "            star_votes.append(vote)\n",
    "\n",
    "        unique_star_names = star_names[0:5]\n",
    "        unique_star_votes = star_votes[0:5]\n",
    "\n",
    "        star_dictionary = dict(zip(unique_star_names, unique_star_votes))\n",
    "    else:\n",
    "        # if no ratings can be found, fill it out with zeros\n",
    "        star_dictionary = dict(zip([5, 4, 3, 2, 1], [0, 0, 0, 0, 0]))\n",
    "\n",
    "    # get the number of reviews\n",
    "    review_number_raw = page.find('span', attrs={'class':'review-headline-count'})\n",
    "    # check if there are any reviews\n",
    "    if review_number_raw:\n",
    "        review_number_as_string = page.find('span', attrs={'class':'review-headline-count'}).text.strip('()')\n",
    "        review_number = int(review_number_as_string)\n",
    "    else:\n",
    "        # if there are no reviews, set the review_number to be zero\n",
    "        review_number = 0\n",
    "\n",
    "    reviews_no = review_number\n",
    "    # generate an output df\n",
    "    stars_and_reviews_df = pd.DataFrame(star_dictionary, index=[0])\n",
    "    stars_and_reviews_df.columns = ['5 stars', '4 stars', '3 stars', '2 stars', '1 star']\n",
    "    stars_and_reviews_df['reviews_no'] = [reviews_no]    \n",
    "\n",
    "    return stars_and_reviews_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multimedia_info(page):\n",
    "    # extract info on photos & video\n",
    "    # input = BeautifulSoup html page for the recipe\n",
    "    # output = multimedia_df with photo count & a yes/no video indicator\n",
    "\n",
    "    video_present = does_recipe_have_video(page)\n",
    "\n",
    "    photo_count = extract_number_of_photos(page)\n",
    "    \n",
    "    multimedia_df = pd.DataFrame({'video_present': video_present, 'photo_count': photo_count}, index=[0])\n",
    "    \n",
    "    return multimedia_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions (wordclouds, save df as Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay() -> None:\n",
    "    # delay for web crawling (so you don't get blocked by the website etc)\n",
    "    \n",
    "    # between 1-3 seconds (allrecipes.com/robots.txt says crawl-delay: 1)\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordclouds(column_name, recipes_df, random_color_func, input_timestamp):\n",
    "    # make wordclouds for specific column in a pandas df\n",
    "    # https://amueller.github.io/word_cloud/index.html\n",
    "    \n",
    "    # input = the name of the column of interest, pandas df with recipe data, recipe_type (cuisine), colouring function\n",
    "    # output = display a wordcloud & save it as a jpeg file\n",
    "    \n",
    "    print(f'\\n*********\\nNow working on {column_name}:')\n",
    "    text = ' '.join(recipes_df[column_name])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='white',\n",
    "                          width=1200,\n",
    "                          height=1000,\n",
    "                          color_func=random_color_func\n",
    "                          ).generate(text)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # save image as file\n",
    "    folder_location = f'/home/bkotryna/ML_practice/allrecipes_project/data/{input_timestamp}/wordclouds'\n",
    "    name_to_save = f\"{folder_location}/{column_name}.jpeg\"\n",
    "    plt.savefig(name_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    # set colours for the wordclouds\n",
    "\n",
    "    # HSL colour system\n",
    "    # can use Colour Picker to choose values\n",
    "    # https://www.google.co.uk/search?q=colour+picker\n",
    "\n",
    "    # code that converts from 256-range colour format (or whatever)    \n",
    "    # h = int(360.0 * 60.0 / 255.0)\n",
    "    # s = int(100.0 * 100.0 / 255.0)\n",
    "    # l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
    "\n",
    "    # my direct way\n",
    "    h = random_state.randint(50, 350) # 0 - 360\n",
    "    s = 60 # 0 - 100\n",
    "    l = random_state.randint(20, 80) # 0 - 100\n",
    "\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nutrition specs into float (strings with calories, g, mg => float)\n",
    "\n",
    "def remove_units(df, column_name, unit):\n",
    "    \n",
    "    print('Now removing units from column: ', column_name)\n",
    "    \n",
    "    # inspect as is\n",
    "    # display(df[column_name].describe())\n",
    "    \n",
    "    # remove units and convert to float\n",
    "    df[column_name] = df[column_name].str.replace(unit, '')\n",
    "    df[column_name] = df[column_name].astype(float)\n",
    "    \n",
    "    # rename the column to include units\n",
    "    column_name_new = f'{column_name} in {unit}'\n",
    "    df = df.rename(columns={column_name: column_name_new})\n",
    "    \n",
    "    # inspect the new version\n",
    "    # display(df[column_name_new].describe())\n",
    "    \n",
    "    # print('******\\n\\n')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NMF\n",
    "# express entries in a text column in terms of its top NMF components\n",
    "\n",
    "def make_nmf_K(col, df, nmf_df, n=4):\n",
    "    # input = df, column of interest, number of NMF components to keep\n",
    "    # output = augmented df that now contains n new columns, each corresponding to an NMF components.\n",
    "    # text from each row in the column of interest is expressed in terms of the NMF components\n",
    "    \n",
    "    print(f\"\\n************\\nNow working on column '{col}':\")\n",
    "    \n",
    "    # obtain data\n",
    "    # cell = a string\n",
    "    data = df[col].to_numpy()\n",
    "    \n",
    "    # tokenise (make into a bag of words)\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(data).todense()\n",
    "    print(f\"vocabulary size: {vectors.shape[1]}\")\n",
    "    print('Tokenising done.')\n",
    "    \n",
    "    # Print vocab items with their frequencies, sorted in descending order by frequency\n",
    "    word_and_frequency_tuples = []\n",
    "    for word, index in vectorizer.vocabulary_.items():\n",
    "        frequency_of_current_word = vectors[:, index].sum()\n",
    "        word_and_frequency_tuples.append((word, frequency_of_current_word))\n",
    "    by_freq = sorted(word_and_frequency_tuples, key=lambda x: x[1], reverse=True)\n",
    "    print(f'Most freqeunt words are:\\n{by_freq[0:5]}')\n",
    "    vocab_size = len(word_and_frequency_tuples)\n",
    "    \n",
    "    # might be useful some time\n",
    "    indices_to_words = {index : word for word, index in vectorizer.vocabulary_.items()}\n",
    "\n",
    "    # do NMF\n",
    "    nmf = NMF(n_components=n)\n",
    "    nmf_projections = nmf.fit_transform(vectors)\n",
    "    #display(nmf_projections)\n",
    "    print('NMF transforming done.')\n",
    "\n",
    "    # Inspect individual components\n",
    "    for component_id in range(len(nmf.components_)):\n",
    "        print(f'\\n***\\nComponent {component_id} for {col}')\n",
    "        \n",
    "        ### Inspect what each component is about (i.e. what the topic is)\n",
    "        print('\\nMost important words are:')\n",
    "        \n",
    "        # BTW nmf.components_ is an np.array\n",
    "        # nmf.components_[component_id]\n",
    "        word_indices_in_descending_order_by_importance_for_component = sorted(\n",
    "            range(vocab_size),\n",
    "            key=(lambda word_index: nmf.components_[component_id, word_index]),\n",
    "            reverse=True\n",
    "        )\n",
    "        for index in word_indices_in_descending_order_by_importance_for_component[:5]:\n",
    "            word = indices_to_words[index]\n",
    "            importance = nmf.components_[component_id, index]\n",
    "            print('{} : {:.3f}'.format(word, importance))\n",
    "\n",
    "        ### find the sentence that has the largest projection along that component\n",
    "        id_of_recipe_with_greatest_projection_along_component = max(\n",
    "            range(len(data)),\n",
    "            key=(lambda recipe_index: nmf_projections[recipe_index, component_id])\n",
    "        )\n",
    "        print('\\nThe entry that most strongly embodies this component is:')\n",
    "        print(data[id_of_recipe_with_greatest_projection_along_component])\n",
    "\n",
    "    # generate column names for nmf df\n",
    "    col_names_list = []\n",
    "    for num in range(1, nmf_projections.shape[1] + 1):\n",
    "        col_name = f\"{col}_nmf_{num}\"\n",
    "        col_names_list.append(col_name)\n",
    "\n",
    "    # generate nmf df\n",
    "    our_col_nmf_df = pd.DataFrame(nmf_projections, columns=col_names_list)\n",
    "    our_col_nmf_df\n",
    "    print('\\n***\\nnp.array made into pd.df')\n",
    "\n",
    "    # set index to match recipe_id\n",
    "    our_col_nmf_df['recipe_id'] = df.index\n",
    "    our_col_nmf_df.set_index('recipe_id', inplace=True)\n",
    "    print(\"Index now reset back to recipe_id.\")\n",
    "    \n",
    "    return our_col_nmf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do stemming, then bag-of-words, then NMF\n",
    "\n",
    "def stem_make_nmf(col, df, nmf_df, n=4):\n",
    "    # input = df, column of interest, number of NMF components to keep\n",
    "    # output = augmented df that now contains n new columns, each corresponding to an NMF components.\n",
    "    # text from each row in the column of interest is expressed in terms of the NMF components\n",
    "    \n",
    "    print(f\"\\n************\\nNow working on column '{col}':\")\n",
    "    \n",
    "    # obtain data\n",
    "    # cell = a string\n",
    "    data = df  \n",
    "    \n",
    "    # let's stem\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    # will generate a list with one item per recipe\n",
    "    # each item will be a string of stemmed words\n",
    "    data['stemmed'] = ''\n",
    "\n",
    "    for index in data.index:\n",
    "        # stem\n",
    "        item = data.loc[index, col]\n",
    "        item_stem = stemmer.stemWords(item.split())\n",
    "\n",
    "        # generate a single string per recipe\n",
    "        data_string = ' '.join(item_stem)\n",
    "\n",
    "        # remove strange quotation marks\n",
    "        # (Ig ideally would know how not to generate them in the first place)\n",
    "        data_string = data_string.replace(\"'\",\"\") \n",
    "        data_string = data_string.replace('\"','')\n",
    "\n",
    "        # append the string to df\n",
    "        data.at[index, 'stemmed'] = data_string\n",
    "    print('Stemming done') \n",
    "    \n",
    "    \n",
    "    data = df['stemmed'].to_numpy()\n",
    "    \n",
    "    \n",
    "    # data = df[col].to_numpy()\n",
    "    # tokenise (make into a bag of words)\n",
    "    \n",
    "    from sklearn.feature_extraction import text \n",
    "\n",
    "    # add extra stop words\n",
    "    extra_stop_words = ['i', 'ii', 'iii']\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(extra_stop_words)\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "    vectors = vectorizer.fit_transform(data).todense()\n",
    "    print(f\"vocabulary size: {vectors.shape[1]}\")\n",
    "    print('Tokenising done.')\n",
    "    \n",
    "    # Print vocab items with their frequencies, sorted in descending order by frequency\n",
    "    word_and_frequency_tuples = []\n",
    "    for word, index in vectorizer.vocabulary_.items():\n",
    "        frequency_of_current_word = vectors[:, index].sum()\n",
    "        word_and_frequency_tuples.append((word, frequency_of_current_word))\n",
    "    by_freq = sorted(word_and_frequency_tuples, key=lambda x: x[1], reverse=True)\n",
    "    print(f'Most freqeunt words are:\\n{by_freq[0:5]}')\n",
    "    vocab_size = len(word_and_frequency_tuples)\n",
    "    \n",
    "    # might be useful some time\n",
    "    indices_to_words = {index : word for word, index in vectorizer.vocabulary_.items()}\n",
    "\n",
    "    # do NMF\n",
    "    nmf = NMF(n_components=n)\n",
    "    nmf_projections = nmf.fit_transform(vectors)\n",
    "    #display(nmf_projections)\n",
    "    print('NMF transforming done.')\n",
    "\n",
    "    # Inspect individual components\n",
    "    for component_id in range(len(nmf.components_)):\n",
    "        print(f'\\n***\\nComponent {component_id} for {col}')\n",
    "        \n",
    "        ### Inspect what each component is about (i.e. what the topic is)\n",
    "        print('\\nMost important words are:')\n",
    "        \n",
    "        # BTW nmf.components_ is an np.array\n",
    "        # nmf.components_[component_id]\n",
    "        word_indices_in_descending_order_by_importance_for_component = sorted(\n",
    "            range(vocab_size),\n",
    "            key=(lambda word_index: nmf.components_[component_id, word_index]),\n",
    "            reverse=True\n",
    "        )\n",
    "        for index in word_indices_in_descending_order_by_importance_for_component[:5]:\n",
    "            word = indices_to_words[index]\n",
    "            importance = nmf.components_[component_id, index]\n",
    "            print('{} : {:.3f}'.format(word, importance))\n",
    "\n",
    "        ### find the sentence that has the largest projection along that component\n",
    "        id_of_recipe_with_greatest_projection_along_component = max(\n",
    "            range(len(data)),\n",
    "            key=(lambda recipe_index: nmf_projections[recipe_index, component_id])\n",
    "        )\n",
    "        print('\\nThe entry that most strongly embodies this component is:')\n",
    "        original_col = df[col].to_numpy()\n",
    "        print(original_col[id_of_recipe_with_greatest_projection_along_component])\n",
    "\n",
    "    # generate column names for nmf df\n",
    "    col_names_list = []\n",
    "    for num in range(nmf_projections.shape[1]):\n",
    "        col_name = f\"{col}_nmf_{num}\"\n",
    "        col_names_list.append(col_name)\n",
    "\n",
    "    # generate nmf df\n",
    "    our_col_nmf_df = pd.DataFrame(nmf_projections, columns=col_names_list)\n",
    "    our_col_nmf_df\n",
    "    print('\\n***\\nnp.array made into pd.df')\n",
    "\n",
    "    # set index to match recipe_id\n",
    "    our_col_nmf_df['recipe_id'] = df.index\n",
    "    our_col_nmf_df.set_index('recipe_id', inplace=True)\n",
    "    print(\"Index now reset back to recipe_id.\")\n",
    "    \n",
    "    return our_col_nmf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_regressor(regressor, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    feature_names = X_train.columns\n",
    "    features_and_importances = []\n",
    "    \n",
    "    for feature_id in range(len(regressor.feature_importances_)):\n",
    "        feature_name = feature_names[feature_id]\n",
    "        feature_importance = regressor.feature_importances_[feature_id]\n",
    "        features_and_importances.append((feature_name, feature_importance))\n",
    "        \n",
    "    features_and_importances.sort(key=(lambda pair: pair[1]), reverse=True)\n",
    "    \n",
    "    for pair in features_and_importances[:10]:\n",
    "        print('Feature {}: importance = {:.3f}'.format(pair[0], pair[1]))\n",
    "    \n",
    "    train_predictions = regressor.predict(X_train)\n",
    "    test_predictions = regressor.predict(X_test)\n",
    "    mean_abs_error_on_train = mean_absolute_error(y_train, train_predictions)\n",
    "    mean_abs_error_on_test = mean_absolute_error(y_test, test_predictions)\n",
    "    print('Mean abs error on train = {:.3f}'.format(mean_abs_error_on_train))\n",
    "    print('Mean abs error on test = {:.3f}'.format(mean_abs_error_on_test))\n",
    "    \n",
    "    mean_sq_error_on_train = mean_squared_error(y_train, train_predictions)\n",
    "    mean_sq_error_on_test = mean_squared_error(y_test, test_predictions)\n",
    "    print('Mean sq error on train = {:.3f}'.format(mean_sq_error_on_train))\n",
    "    print('Mean sq error on test = {:.3f}'.format(mean_sq_error_on_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used currently, but seem useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_retrieved_html_pages(page_list):\n",
    "    # save downloaded html pages\n",
    "    # input = a list of recipe html's for different recipe types (cuisines)\n",
    "    # output = individual html files saved on the local machine\n",
    "    \n",
    "    time_now = datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "    content_type = 'pages'\n",
    "    extension = 'html'\n",
    "    name_to_save = f\"./data/{time_now}_recipe_pages_html\"\n",
    "\n",
    "    with open(name_to_save, 'w') as f:\n",
    "        for page in page_list:\n",
    "            f.write(\"%s\\n\" % page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old / backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# express entries in a text column in terms of its top NMF components\n",
    "\n",
    "def make_nmf_K_backup(col, df, nmf_df, n=4):\n",
    "    # input = df, column of interest, number of NMF components to keep\n",
    "    # output = augmented df that now contains n new columns, each corresponding to an NMF components.\n",
    "    # text from each row in the column of interest is expressed in terms of the NMF components\n",
    "    \n",
    "    print(f\"\\n\\nNow working on column '{col}':\")\n",
    "    \n",
    "    # obtain data\n",
    "    # cell = a string\n",
    "    # we reset index since we'll be arranging things by index or sth\n",
    "    df = df.reset_index(drop=True)\n",
    "    data = df[col].to_numpy()\n",
    "    #display(data)\n",
    "    \n",
    "    # tokenise (make into a bag of words)\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(data).todense()\n",
    "    print(f\"vocabulary size: {vectors.shape[1]}\")\n",
    "    print('Tokenising done')\n",
    "    \n",
    "    # Print vocab items with their frequencies, sorted in descending order by frequency\n",
    "    word_and_frequency_tuples = []\n",
    "    for word, index in vectorizer.vocabulary_.items():\n",
    "        frequency_of_current_word = vectors[:, index].sum()\n",
    "        word_and_frequency_tuples.append((word, frequency_of_current_word))\n",
    "    by_freq = sorted(word_and_frequency_tuples, key=lambda x: x[1], reverse=True)\n",
    "    print(f'Most freqeunt words are\\n{by_freq[0:5]}')\n",
    "\n",
    "    vocab_size = len(word_and_frequency_tuples)\n",
    "    print('Number of words in vocab = {}'.format(vocab_size))\n",
    "\n",
    "    indices_to_words = {index : word for word, index in vectorizer.vocabulary_.items()}\n",
    "\n",
    "    # do NMF\n",
    "    nmf = NMF(n_components=n)\n",
    "    nmf_projections = nmf.fit_transform(vectors)\n",
    "    #display(nmf_projections)\n",
    "    print('NMF transforming done')\n",
    "\n",
    "    # Inspect what each component is about (i.e. what the topic is)\n",
    "    for component_id in range(len(nmf.components_)):\n",
    "        print(f'\\nComponent {component_id} for {col}')\n",
    "        # BTW nmf.components_ is an np.array\n",
    "        # nmf.components_[component_id]\n",
    "        word_indices_in_descending_order_by_importance_for_component = sorted(\n",
    "            range(vocab_size),\n",
    "            key=(lambda word_index: nmf.components_[component_id, word_index]),\n",
    "            reverse=True\n",
    "        )\n",
    "        for index in word_indices_in_descending_order_by_importance_for_component[:5]:\n",
    "            word = indices_to_words[index]\n",
    "            importance = nmf.components_[component_id, index]\n",
    "            print('{} : {:.3f}'.format(word, importance))\n",
    "\n",
    "    # For each component, find the sentence that has the largest projection along that component\n",
    "    for component_id in range(len(nmf.components_)):\n",
    "        id_of_recipe_with_greatest_projection_along_component = max(\n",
    "            range(len(data)),\n",
    "            key=(lambda recipe_index: nmf_projections[recipe_index, component_id])\n",
    "        )\n",
    "        print(f'\\nComponent {component_id} for {col}')\n",
    "        print('The entry that most strongly embodies this component is:')\n",
    "        print(data[id_of_recipe_with_greatest_projection_along_component])\n",
    "\n",
    "    # generate column names for nmf df\n",
    "    col_names_list = []\n",
    "    for num in range(1, nmf_projections.shape[1] + 1):\n",
    "        col_name = f\"{col}_nmf_{num}\"\n",
    "        col_names_list.append(col_name)\n",
    "\n",
    "    # generate nmf df\n",
    "    our_col_nmf_df = pd.DataFrame(nmf_projections, columns=col_names_list)\n",
    "    our_col_nmf_df\n",
    "    print('\\nnp made into pd.df')\n",
    "\n",
    "    # set index to match recipe_id\n",
    "    our_col_nmf_df['recipe_id'] = df.index\n",
    "    our_col_nmf_df.set_index('recipe_id', inplace=True)\n",
    "    print(\"\\nSet_index done.\")\n",
    "\n",
    "    # add the nmf columns to the master nmf_df\n",
    "    nmf_df = pd.concat([nmf_df, our_col_nmf_df], axis=1)\n",
    "    print('\\nMaster df updated.')\n",
    "    \n",
    "    return nmf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# express entries in a text column in terms of its top NMF components\n",
    "\n",
    "def make_nmf_old(col, df, nmf_df, n=15):\n",
    "    # input = df, column of interest, number of NMF components to keep\n",
    "    # output = augmented df that now contains n new columns, each corresponding to an NMF components.\n",
    "    # text from each row in the column of interest is expressed in terms of the NMF components\n",
    "    \n",
    "    print(f\"\\n\\nNow working on column '{col}':\")\n",
    "    \n",
    "    # obtain data\n",
    "    # cell = a string\n",
    "    our_col = df[col]\n",
    "    \n",
    "    # let's stem\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    # will generate a list with one item per recipe\n",
    "    # each item will be a string of stemmed words\n",
    "    our_col_stem = []\n",
    "\n",
    "    for item in our_col:\n",
    "        # stem\n",
    "        item_stem = stemmer.stemWords(item.split())\n",
    "\n",
    "        # generate a single string per recipe\n",
    "        our_col_string = ' '.join(item_stem)\n",
    "\n",
    "        # remove strange quotation marks\n",
    "        # (Ig ideally would know how not to generate them in the first place)\n",
    "        our_col_string = our_col_string.replace(\"'\",\"\") \n",
    "        our_col_string = our_col_string.replace('\"','')\n",
    "\n",
    "        # append the string to master list (one item per recipe)\n",
    "        our_col_stem.append(our_col_string)\n",
    "    print('Stemming done') \n",
    "    \n",
    "    # tokenise (make into a bag of words)\n",
    "    count_vect = CountVectorizer(stop_words='english')\n",
    "    our_col_counts = count_vect.fit_transform(our_col_stem)\n",
    "    print(f\"vocabulary size: {our_col_counts.shape[1]}\")\n",
    "    print('Tokenising done')   \n",
    "    \n",
    "    # tf-idf\n",
    "    our_col_tf = TfidfTransformer(use_idf=False).fit_transform(our_col_counts)\n",
    "    print(\"TF-IDF done\")\n",
    "    \n",
    "    # NMF\n",
    "    nmf = NMF(n_components=n, random_state=0)\n",
    "    # fit NMF model to tf-idf output\n",
    "    nmf.fit(our_col_tf)\n",
    "\n",
    "    # express ingredients per recipe via NMF components\n",
    "    our_col_nmf = nmf.transform(our_col_tf)\n",
    "    print('NMF transforming done')\n",
    "\n",
    "    # generate column names for nmf df\n",
    "    col_names_list = []\n",
    "    for num in range(1, our_col_nmf.shape[1] + 1):\n",
    "        col_name = f\"{col}_nmf_{num}\"\n",
    "        col_names_list.append(col_name)\n",
    "\n",
    "    # generate nmf df\n",
    "    our_col_nmf_df = pd.DataFrame(our_col_nmf, columns=col_names_list)\n",
    "    our_col_nmf_df\n",
    "    print('np made into pd.df')\n",
    "\n",
    "    # set index to match recipe_id\n",
    "    our_col_nmf_df['recipe_id'] = df.index\n",
    "    our_col_nmf_df.set_index('recipe_id', inplace=True)\n",
    "    print(\"Set_index done\")\n",
    "\n",
    "    # add the nmf columns to the master nmf_df\n",
    "    nmf_df = pd.concat([nmf_df, our_col_nmf_df], axis=1)\n",
    "    print('Master df updated')\n",
    "    \n",
    "    return nmf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
